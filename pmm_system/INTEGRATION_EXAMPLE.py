#!/usr/bin/env python3
"""
Post-Market Monitoring Integration Example
==========================================
æ¼”ç¤ºå¦‚ä½•é›†æˆ PMM ç³»ç»Ÿä¸ç°æœ‰çš„ health-safety skills

è¿™ä¸ªæ–‡ä»¶å±•ç¤ºäº†å®Œæ•´çš„é›†æˆæµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
1. PMM Core Agent åˆå§‹åŒ–
2. ä¸ ai-safety-planning é›†æˆ
3. ä¸ ai-ethics-advisor é›†æˆï¼ˆå¤ç”¨ bias-monitoring.pyï¼‰
4. ä¸ incident-responder é›†æˆ
5. å®Œæ•´çš„ç›‘æ§æµç¨‹ç¤ºä¾‹
"""

import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from dataclasses import dataclass
from pathlib import Path
import sys

# æ·»åŠ  skills æ¨¡å—åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent / "skills/explaining-code/health-safety"))

# å¯¼å…¥ç°æœ‰çš„ bias monitoring æ¨¡å—
from skills.explaining_code.health_safety.ai_ethics_advisor.modules.technical_safeguards.bias_monitoring import (
    BiasMonitor,
    compute_fairness_metrics,
    check_four_fifths_rule
)


# ============================================================================
# æ•°æ®æ¨¡å‹
# ============================================================================

@dataclass
class AIInteraction:
    """AI ç³»ç»Ÿäº¤äº’è®°å½•"""
    interaction_id: str
    timestamp: datetime
    user_id: Optional[str]
    prompt: str
    response: str
    response_time: float
    model_version: str
    metadata: Dict
    demographics: Optional[Dict] = None  # ç”¨äºåè§ç›‘æ§


@dataclass
class SafetyThreshold:
    """å®‰å…¨é˜ˆå€¼é…ç½®"""
    metric_name: str
    target: float
    alert_threshold: float
    critical_threshold: float


@dataclass
class MonitoringAlert:
    """ç›‘æ§å‘Šè­¦"""
    alert_id: str
    timestamp: datetime
    alert_type: str
    severity: str  # critical, high, medium, low
    metric_name: str
    current_value: float
    threshold: float
    details: Dict


# ============================================================================
# Integration 1: SafetyMetricsProvider - ä¸ ai-safety-planning é›†æˆ
# ============================================================================

class SafetyMetricsProvider:
    """
    ä» ai-safety-planning åŠ è½½å®‰å…¨æŒ‡æ ‡é…ç½®

    åŸºäº: skills/health-safety/ai-safety-SKILL.md
    ç« èŠ‚: Safety Evaluation Metrics (è¡Œ 418-428)
    """

    def __init__(self):
        self.thresholds = self._load_default_thresholds()

    def _load_default_thresholds(self) -> Dict[str, SafetyThreshold]:
        """
        åŠ è½½é»˜è®¤çš„å®‰å…¨é˜ˆå€¼

        è¿™äº›é˜ˆå€¼æ¥è‡ª ai-safety-SKILL.md çš„æ ‡å‡†å®šä¹‰
        """
        return {
            'refusal_rate': SafetyThreshold(
                metric_name='refusal_rate',
                target=0.99,
                alert_threshold=0.95,
                critical_threshold=0.90
            ),
            'false_positive_rate': SafetyThreshold(
                metric_name='false_positive_rate',
                target=0.05,
                alert_threshold=0.10,
                critical_threshold=0.15
            ),
            'jailbreak_resistance': SafetyThreshold(
                metric_name='jailbreak_resistance',
                target=0.95,
                alert_threshold=0.90,
                critical_threshold=0.85
            ),
            'toxicity_score': SafetyThreshold(
                metric_name='toxicity_score',
                target=0.1,
                alert_threshold=0.2,
                critical_threshold=0.3
            ),
            'bias_score': SafetyThreshold(
                metric_name='bias_score',
                target=0.9,
                alert_threshold=0.8,
                critical_threshold=0.7
            ),
            # MYTASK.md å®šä¹‰çš„æŒ‡æ ‡
            'response_accuracy': SafetyThreshold(
                metric_name='response_accuracy',
                target=0.95,
                alert_threshold=0.90,
                critical_threshold=0.85
            ),
            'hallucination_rate': SafetyThreshold(
                metric_name='hallucination_rate',
                target=0.05,
                alert_threshold=0.10,
                critical_threshold=0.15
            ),
            'user_satisfaction': SafetyThreshold(
                metric_name='user_satisfaction',
                target=4.0,
                alert_threshold=3.5,
                critical_threshold=3.0
            ),
        }

    def get_threshold(self, metric_name: str) -> SafetyThreshold:
        """è·å–æŒ‡å®šæŒ‡æ ‡çš„é˜ˆå€¼"""
        return self.thresholds.get(metric_name)

    def check_threshold(self, metric_name: str, value: float) -> Optional[str]:
        """
        æ£€æŸ¥æŒ‡æ ‡æ˜¯å¦è¿åé˜ˆå€¼

        Returns:
            None if within threshold
            'alert' if alert threshold breached
            'critical' if critical threshold breached
        """
        threshold = self.get_threshold(metric_name)
        if not threshold:
            return None

        # æ ¹æ®æŒ‡æ ‡ç±»å‹åˆ¤æ–­ï¼ˆæœ‰äº›æŒ‡æ ‡æ˜¯è¶Šé«˜è¶Šå¥½ï¼Œæœ‰äº›æ˜¯è¶Šä½è¶Šå¥½ï¼‰
        higher_is_better = metric_name in [
            'refusal_rate', 'jailbreak_resistance',
            'bias_score', 'response_accuracy', 'user_satisfaction'
        ]

        if higher_is_better:
            if value < threshold.critical_threshold:
                return 'critical'
            elif value < threshold.alert_threshold:
                return 'alert'
        else:
            if value > threshold.critical_threshold:
                return 'critical'
            elif value > threshold.alert_threshold:
                return 'alert'

        return None


# ============================================================================
# Integration 2: EthicsMonitoringBridge - ä¸ ai-ethics-advisor é›†æˆ
# ============================================================================

class EthicsMonitoringBridge:
    """
    ä¸ ai-ethics-advisor çš„åŒå‘é›†æˆ

    å¤ç”¨: skills/health-safety/ai-ethics-advisor/modules/technical-safeguards/bias-monitoring.py
    """

    def __init__(self, protected_attributes: List[str] = None):
        """
        åˆå§‹åŒ–ä¼¦ç†ç›‘æ§æ¡¥

        Args:
            protected_attributes: éœ€è¦ç›‘æ§çš„å—ä¿æŠ¤å±æ€§
        """
        if protected_attributes is None:
            protected_attributes = ['gender', 'race', 'age', 'disability']

        # å¤ç”¨ç°æœ‰çš„ BiasMonitor
        self.bias_monitor = BiasMonitor(
            protected_attributes=protected_attributes,
            alert_thresholds={
                'demographic_parity': 0.8,  # 4/5ths rule
                'tpr_disparity': 0.05,
                'fpr_disparity': 0.05
            },
            window_size=1000
        )

        self.alert_history = []

    def log_interaction(self, interaction: AIInteraction):
        """
        è®°å½•äº¤äº’ç”¨äºåè§ç›‘æ§

        Args:
            interaction: AI äº¤äº’è®°å½•
        """
        if not interaction.demographics:
            return

        # è®°å½•é¢„æµ‹ï¼ˆå‡è®¾å“åº”ä¸ä¸ºç©ºè§†ä¸ºæ­£é¢é¢„æµ‹ï¼‰
        prediction = 1 if interaction.response else 0

        self.bias_monitor.log_prediction(
            prediction=prediction,
            ground_truth=None,  # é€šå¸¸éœ€è¦åç»­æ ‡æ³¨
            attributes=interaction.demographics
        )

    def check_bias_alerts(self) -> List[Dict]:
        """
        æ£€æŸ¥åè§å‘Šè­¦

        Returns:
            å½“å‰æ´»è·ƒçš„åè§å‘Šè­¦åˆ—è¡¨
        """
        alerts = self.bias_monitor.check_alerts()

        # è®°å½•å‘Šè­¦å†å²
        for alert in alerts:
            alert['detected_at'] = datetime.utcnow()
            self.alert_history.append(alert)

        return alerts

    def get_bias_report(self) -> str:
        """
        ç”Ÿæˆåè§ç›‘æ§æŠ¥å‘Š

        Returns:
            æ ¼å¼åŒ–çš„æŠ¥å‘Šå­—ç¬¦ä¸²
        """
        return self.bias_monitor.get_summary_report()

    def should_trigger_ethics_assessment(self, alerts: List[Dict]) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥è§¦å‘å®Œæ•´çš„ä¼¦ç†è¯„ä¼°

        åŸºäº: ai-ethics-advisor/SKILL.md
        - Tier 1: Rapid Screen (15-30 min) - ç”¨äºè½»å¾®é—®é¢˜
        - Tier 2: Comprehensive Assessment (2-4 hours) - ç”¨äºä¸¥é‡é—®é¢˜

        Args:
            alerts: å½“å‰å‘Šè­¦åˆ—è¡¨

        Returns:
            True if comprehensive assessment needed
        """
        # å¦‚æœæœ‰ä»»ä½• demographic_parity è¿è§„ï¼Œè§¦å‘è¯„ä¼°
        for alert in alerts:
            if alert['type'] == 'demographic_parity':
                return True

        # å¦‚æœå¤šä¸ªç¾¤ä½“åŒæ—¶å‡ºç°é—®é¢˜ï¼Œè§¦å‘è¯„ä¼°
        affected_groups = set()
        for alert in alerts:
            if 'group' in alert:
                affected_groups.add(alert['group'])

        return len(affected_groups) >= 2

    async def trigger_ethics_assessment(self, alerts: List[Dict]) -> Dict:
        """
        è§¦å‘ä¼¦ç†è¯„ä¼°

        Args:
            alerts: è§¦å‘è¯„ä¼°çš„å‘Šè­¦åˆ—è¡¨

        Returns:
            è¯„ä¼°ç»“æœ
        """
        # åˆ¤æ–­ä½¿ç”¨å“ªä¸ªå±‚çº§çš„è¯„ä¼°
        if len(alerts) > 3 or any(a.get('severity') == 'critical' for a in alerts):
            assessment_tier = 'tier2_comprehensive'
            print("ğŸ”´ Triggering Tier 2 Comprehensive Ethics Assessment")
        else:
            assessment_tier = 'tier1_rapid'
            print("ğŸŸ¡ Triggering Tier 1 Rapid Ethics Screen")

        # åœ¨å®é™…ç³»ç»Ÿä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨ ai-ethics-advisor çš„ API
        # è¿™é‡Œæˆ‘ä»¬è¿”å›ä¸€ä¸ªæ¨¡æ‹Ÿç»“æœ
        return {
            'assessment_tier': assessment_tier,
            'triggered_at': datetime.utcnow(),
            'alerts_count': len(alerts),
            'recommendation': 'Review model training data and implement bias mitigation',
            'action_required': True
        }


# ============================================================================
# Integration 3: IncidentTrigger - ä¸ incident-responder é›†æˆ
# ============================================================================

class IncidentTrigger:
    """
    ä¸ incident-responder é›†æˆ

    åŸºäº: skills/health-safety/skills/incident-responder/SKILL.md
    """

    # ä¸¥é‡çº§åˆ«æ˜ å°„åˆ°å“åº”æ—¶é—´
    SEVERITY_MAPPING = {
        'critical': {
            'priority': 'P1',
            'response_time': '< 5 minutes',
            'classification': 'security_breach'
        },
        'high': {
            'priority': 'P2',
            'response_time': '< 1 hour',
            'classification': 'compliance_violation'
        },
        'medium': {
            'priority': 'P3',
            'response_time': '< 24 hours',
            'classification': 'service_outage'
        },
        'low': {
            'priority': 'P4',
            'response_time': '< 1 week',
            'classification': 'data_incident'
        }
    }

    def __init__(self):
        self.incident_counter = 0
        self.active_incidents = []

    def create_incident(self,
                       alert: MonitoringAlert,
                       context: Dict = None) -> str:
        """
        åˆ›å»ºäº‹ä»¶å¹¶è§¦å‘å“åº”

        åŸºäº incident-responder/SKILL.md çš„ First response procedures (è¡Œ 36-44):
        - Initial assessment
        - Severity determination
        - Team mobilization
        - Containment actions
        - Evidence preservation

        Args:
            alert: ç›‘æ§å‘Šè­¦
            context: é¢å¤–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯

        Returns:
            incident_id
        """
        self.incident_counter += 1
        incident_id = f"INC-{datetime.utcnow().strftime('%Y%m%d')}-{self.incident_counter:04d}"

        severity_config = self.SEVERITY_MAPPING.get(alert.severity, self.SEVERITY_MAPPING['medium'])

        incident = {
            'incident_id': incident_id,
            'created_at': datetime.utcnow(),
            'alert': alert,
            'priority': severity_config['priority'],
            'classification': severity_config['classification'],
            'response_time_sla': severity_config['response_time'],
            'status': 'open',
            'context': context or {},
            'actions_taken': []
        }

        self.active_incidents.append(incident)

        print(f"\n{'='*60}")
        print(f"ğŸš¨ INCIDENT CREATED: {incident_id}")
        print(f"{'='*60}")
        print(f"Priority: {severity_config['priority']}")
        print(f"Classification: {severity_config['classification']}")
        print(f"Response Time SLA: {severity_config['response_time']}")
        print(f"Alert Type: {alert.alert_type}")
        print(f"Severity: {alert.severity}")
        print(f"Metric: {alert.metric_name} = {alert.current_value:.3f}")
        print(f"Threshold: {alert.threshold:.3f}")
        print(f"{'='*60}\n")

        # åœ¨å®é™…ç³»ç»Ÿä¸­ï¼Œè¿™é‡Œä¼šï¼š
        # 1. é€šçŸ¥å“åº”å›¢é˜Ÿ
        # 2. åˆ›å»º PagerDuty/Opsgenie å‘Šè­¦
        # 3. å‘é€ Slack é€šçŸ¥
        # 4. å¯åŠ¨å“åº”æµç¨‹

        return incident_id

    def get_active_incidents(self) -> List[Dict]:
        """è·å–æ´»è·ƒäº‹ä»¶åˆ—è¡¨"""
        return [inc for inc in self.active_incidents if inc['status'] == 'open']

    def close_incident(self, incident_id: str, resolution: str):
        """å…³é—­äº‹ä»¶"""
        for incident in self.active_incidents:
            if incident['incident_id'] == incident_id:
                incident['status'] = 'closed'
                incident['closed_at'] = datetime.utcnow()
                incident['resolution'] = resolution
                print(f"âœ“ Incident {incident_id} closed: {resolution}")
                break


# ============================================================================
# PMM Core Agent - ä¸»åè°ƒå™¨
# ============================================================================

class PMMCoreAgent:
    """
    Post-Market Monitoring æ ¸å¿ƒä»£ç†

    åè°ƒæ‰€æœ‰ç›‘æ§åŠŸèƒ½å’Œé›†æˆ
    """

    def __init__(self):
        self.safety_provider = SafetyMetricsProvider()
        self.ethics_bridge = EthicsMonitoringBridge()
        self.incident_trigger = IncidentTrigger()

        # ç›‘æ§æ•°æ®
        self.interactions_log = []
        self.metrics_buffer = {
            'response_accuracy': [],
            'hallucination_rate': [],
            'user_satisfaction': [],
        }

    async def process_interaction(self, interaction: AIInteraction):
        """
        å¤„ç†å•ä¸ª AI äº¤äº’

        æµç¨‹:
        1. è®°å½•äº¤äº’
        2. æå–æŒ‡æ ‡
        3. æ£€æŸ¥é˜ˆå€¼
        4. å¦‚æœæœ‰äººå£ç»Ÿè®¡ä¿¡æ¯ï¼Œè®°å½•åˆ°åè§ç›‘æ§
        5. è§¦å‘å‘Šè­¦ï¼ˆå¦‚æœéœ€è¦ï¼‰
        """
        print(f"\nğŸ“ Processing interaction: {interaction.interaction_id}")

        # 1. è®°å½•äº¤äº’
        self.interactions_log.append(interaction)

        # 2. æå–æŒ‡æ ‡ï¼ˆè¿™é‡Œæ˜¯æ¨¡æ‹Ÿè®¡ç®—ï¼‰
        metrics = await self._extract_metrics(interaction)

        # 3. æ›´æ–°æŒ‡æ ‡ç¼“å†²åŒº
        for metric_name, value in metrics.items():
            if metric_name in self.metrics_buffer:
                self.metrics_buffer[metric_name].append(value)

        # 4. æ£€æŸ¥é˜ˆå€¼
        alerts = []
        for metric_name, value in metrics.items():
            violation = self.safety_provider.check_threshold(metric_name, value)
            if violation:
                alert = MonitoringAlert(
                    alert_id=f"ALT-{datetime.utcnow().timestamp()}",
                    timestamp=datetime.utcnow(),
                    alert_type=f"{metric_name}_threshold_violation",
                    severity=violation,
                    metric_name=metric_name,
                    current_value=value,
                    threshold=self.safety_provider.get_threshold(metric_name).alert_threshold,
                    details={'interaction_id': interaction.interaction_id}
                )
                alerts.append(alert)
                print(f"âš ï¸  Alert: {metric_name} = {value:.3f} (threshold: {alert.threshold:.3f})")

        # 5. åè§ç›‘æ§ï¼ˆå¦‚æœæœ‰äººå£ç»Ÿè®¡ä¿¡æ¯ï¼‰
        if interaction.demographics:
            self.ethics_bridge.log_interaction(interaction)
            bias_alerts = self.ethics_bridge.check_bias_alerts()

            if bias_alerts:
                print(f"âš ï¸  Bias alerts detected: {len(bias_alerts)}")

                # åˆ¤æ–­æ˜¯å¦éœ€è¦è§¦å‘ä¼¦ç†è¯„ä¼°
                if self.ethics_bridge.should_trigger_ethics_assessment(bias_alerts):
                    assessment = await self.ethics_bridge.trigger_ethics_assessment(bias_alerts)
                    print(f"ğŸ“Š Ethics assessment completed: {assessment['recommendation']}")

        # 6. è§¦å‘äº‹ä»¶ï¼ˆå¦‚æœæ˜¯ä¸¥é‡å‘Šè­¦ï¼‰
        for alert in alerts:
            if alert.severity == 'critical':
                incident_id = self.incident_trigger.create_incident(
                    alert=alert,
                    context={
                        'user_id': interaction.user_id,
                        'model_version': interaction.model_version,
                        'recent_interactions': len(self.interactions_log)
                    }
                )

    async def _extract_metrics(self, interaction: AIInteraction) -> Dict[str, float]:
        """
        ä»äº¤äº’ä¸­æå–æŒ‡æ ‡

        åœ¨å®é™…ç³»ç»Ÿä¸­ï¼Œè¿™ä¼šåŒ…æ‹¬ï¼š
        - å“åº”å‡†ç¡®æ€§æ£€æŸ¥
        - å¹»è§‰æ£€æµ‹
        - å¼•ç”¨éªŒè¯
        - æƒ…æ„Ÿåˆ†æ
        ç­‰ç­‰

        è¿™é‡Œæˆ‘ä»¬è¿”å›æ¨¡æ‹Ÿå€¼
        """
        # æ¨¡æ‹ŸæŒ‡æ ‡è®¡ç®—
        import random

        return {
            'response_accuracy': random.uniform(0.85, 0.98),
            'hallucination_rate': random.uniform(0.02, 0.12),
            'user_satisfaction': random.uniform(3.0, 4.8),
        }

    def generate_summary_report(self) -> str:
        """ç”Ÿæˆç›‘æ§æ‘˜è¦æŠ¥å‘Š"""
        report = []
        report.append("\n" + "="*60)
        report.append("POST-MARKET MONITORING SUMMARY REPORT")
        report.append("="*60)
        report.append(f"Report Generated: {datetime.utcnow().isoformat()}")
        report.append(f"Total Interactions: {len(self.interactions_log)}")
        report.append("")

        # æŒ‡æ ‡æ‘˜è¦
        report.append("METRICS SUMMARY:")
        report.append("-"*60)
        for metric_name, values in self.metrics_buffer.items():
            if values:
                avg = sum(values) / len(values)
                threshold = self.safety_provider.get_threshold(metric_name)
                status = "âœ“" if self.safety_provider.check_threshold(metric_name, avg) is None else "âœ—"
                report.append(f"{status} {metric_name}: {avg:.3f} (target: {threshold.target:.3f})")
        report.append("")

        # åè§æŠ¥å‘Š
        report.append("BIAS MONITORING:")
        report.append("-"*60)
        bias_report = self.ethics_bridge.get_bias_report()
        report.append(bias_report)
        report.append("")

        # æ´»è·ƒäº‹ä»¶
        active_incidents = self.incident_trigger.get_active_incidents()
        report.append(f"ACTIVE INCIDENTS: {len(active_incidents)}")
        report.append("-"*60)
        for incident in active_incidents:
            report.append(f"  - {incident['incident_id']}: {incident['alert'].alert_type}")

        report.append("="*60)

        return "\n".join(report)


# ============================================================================
# æ¼”ç¤ºä¸»å‡½æ•°
# ============================================================================

async def demo_pmm_system():
    """æ¼”ç¤º PMM ç³»ç»Ÿçš„å®Œæ•´æµç¨‹"""

    print("\n" + "="*70)
    print("POST-MARKET MONITORING SYSTEM - INTEGRATION DEMO")
    print("="*70)
    print("\nThis demo shows how PMM integrates with:")
    print("1. ai-safety-planning (metrics and thresholds)")
    print("2. ai-ethics-advisor (bias monitoring)")
    print("3. incident-responder (incident management)")
    print("\n" + "="*70 + "\n")

    # åˆå§‹åŒ– PMM Agent
    pmm = PMMCoreAgent()

    # æ¨¡æ‹Ÿä¸€ç³»åˆ— AI äº¤äº’
    demographics_options = [
        {'gender': 'male', 'age_group': '25-35', 'ethnicity': 'caucasian'},
        {'gender': 'female', 'age_group': '35-45', 'ethnicity': 'asian'},
        {'gender': 'male', 'age_group': '45-55', 'ethnicity': 'african_american'},
        {'gender': 'female', 'age_group': '25-35', 'ethnicity': 'hispanic'},
        None  # æœ‰äº›äº¤äº’å¯èƒ½æ²¡æœ‰äººå£ç»Ÿè®¡ä¿¡æ¯
    ]

    print("Simulating 20 AI interactions...\n")

    for i in range(20):
        interaction = AIInteraction(
            interaction_id=f"int_{i+1:03d}",
            timestamp=datetime.utcnow(),
            user_id=f"user_{i % 5 + 1}",
            prompt=f"Sample query {i+1}",
            response=f"Sample response {i+1}",
            response_time=0.5 + (i % 5) * 0.1,
            model_version="gpt-4",
            metadata={'temperature': 0.7},
            demographics=demographics_options[i % len(demographics_options)]
        )

        await pmm.process_interaction(interaction)

        # æ¯ 5 ä¸ªäº¤äº’åæš‚åœä¸€ä¸‹
        if (i + 1) % 5 == 0:
            await asyncio.sleep(0.5)
            print(f"\n{'â”€'*60}")
            print(f"Processed {i+1}/20 interactions")
            print(f"{'â”€'*60}\n")

    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    print("\n" + "="*70)
    print("GENERATING FINAL REPORT...")
    print("="*70)

    report = pmm.generate_summary_report()
    print(report)

    # æ˜¾ç¤ºæ´»è·ƒäº‹ä»¶è¯¦æƒ…
    active_incidents = pmm.incident_trigger.get_active_incidents()
    if active_incidents:
        print("\n" + "="*70)
        print("ACTIVE INCIDENTS DETAILS")
        print("="*70)
        for incident in active_incidents:
            print(f"\nIncident ID: {incident['incident_id']}")
            print(f"Priority: {incident['priority']}")
            print(f"Created: {incident['created_at'].isoformat()}")
            print(f"Alert Type: {incident['alert'].alert_type}")
            print(f"Severity: {incident['alert'].severity}")
            print(f"Response SLA: {incident['response_time_sla']}")


# ============================================================================
# è¿è¡Œæ¼”ç¤º
# ============================================================================

if __name__ == "__main__":
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     POST-MARKET MONITORING INTEGRATION EXAMPLE                   â•‘
â•‘     EU AI Act Article 72 Compliance                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # è¿è¡Œå¼‚æ­¥æ¼”ç¤º
    asyncio.run(demo_pmm_system())

    print("\n" + "="*70)
    print("DEMO COMPLETED")
    print("="*70)
    print("\nNext Steps:")
    print("1. Review POST_MARKET_MONITORING_DESIGN.md for full architecture")
    print("2. Read QUICK_START_GUIDE.md for deployment instructions")
    print("3. Explore the API documentation at http://localhost:8000/docs")
    print("\n" + "="*70 + "\n")
